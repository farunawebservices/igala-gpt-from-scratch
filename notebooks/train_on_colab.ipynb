{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a3c7e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Upload Files\n",
    "!pip install tokenizers -q\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üì§ Upload these files from your local machine:\")\n",
    "print(\"  1. data/igala_corpus.txt\")\n",
    "print(\"  2. outputs/tokenizer/igala_tokenizer.json\")\n",
    "print(\"  3. scripts/model.py\")\n",
    "print(\"\\nClick 'Choose Files' below:\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Organize files\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('outputs/tokenizer', exist_ok=True)\n",
    "os.makedirs('scripts', exist_ok=True)\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    if 'corpus' in filename:\n",
    "        !mv {filename} data/igala_corpus.txt\n",
    "    elif 'tokenizer' in filename:\n",
    "        !mv {filename} outputs/tokenizer/igala_tokenizer.json\n",
    "    elif 'model.py' in filename:\n",
    "        !mv {filename} scripts/model.py\n",
    "\n",
    "print(\"‚úÖ Files uploaded!\")\n",
    "\n",
    "# Cell 2: Training Script\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('scripts')\n",
    "from model import IgalaGPT, GPTConfig\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"outputs/tokenizer/igala_tokenizer.json\")\n",
    "print(f\"üìñ Tokenizer loaded: {tokenizer.get_vocab_size()} tokens\")\n",
    "\n",
    "# Load corpus\n",
    "with open('data/igala_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "print(f\"üìö Corpus: {len(corpus)} characters, {len(corpus.split())} words\")\n",
    "\n",
    "# Tokenize entire corpus\n",
    "tokens = tokenizer.encode(corpus).ids\n",
    "print(f\"üî¢ Total tokens: {len(tokens)}\")\n",
    "\n",
    "# Dataset\n",
    "class IgalaDataset(Dataset):\n",
    "    def __init__(self, tokens, block_size):\n",
    "        self.tokens = tokens\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.tokens[idx:idx + self.block_size + 1]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Config\n",
    "config = GPTConfig()\n",
    "config.vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "# Create model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "\n",
    "model = IgalaGPT(config).to(device)\n",
    "print(f\"üß† Model: {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters\")\n",
    "\n",
    "# Training setup\n",
    "dataset = IgalaDataset(tokens, config.block_size)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(dataloader)*10)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "training_logs = []\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch_idx, (x, y) in enumerate(pbar):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(x, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    training_logs.append({'epoch': epoch+1, 'loss': avg_loss})\n",
    "    print(f\"‚úÖ Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Generate sample\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        model.eval()\n",
    "        prompt = \"<BOS>\"\n",
    "        prompt_tokens = torch.tensor([tokenizer.encode(prompt).ids], dtype=torch.long).to(device)\n",
    "        generated = model.generate(prompt_tokens, max_new_tokens=30, temperature=0.8, top_k=40)\n",
    "        generated_text = tokenizer.decode(generated[0].tolist())\n",
    "        print(f\"üéØ Sample generation: {generated_text}\\n\")\n",
    "\n",
    "print(\"‚úÖ Training complete!\")\n",
    "\n",
    "# Cell 3: Save Model\n",
    "os.makedirs('outputs/model_checkpoints', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'training_logs': training_logs\n",
    "}, 'outputs/model_checkpoints/igala_gpt_final.pt')\n",
    "\n",
    "# Save logs\n",
    "with open('outputs/training_logs.json', 'w') as f:\n",
    "    json.dump(training_logs, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Model saved!\")\n",
    "\n",
    "# Cell 4: Download Trained Model\n",
    "from google.colab import files\n",
    "\n",
    "files.download('outputs/model_checkpoints/igala_gpt_final.pt')\n",
    "files.download('outputs/training_logs.json')\n",
    "\n",
    "print(\"‚úÖ Download complete! Transfer these to your local outputs/ folder\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
